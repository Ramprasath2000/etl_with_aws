{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i8hjgKS5Evh",
        "outputId": "0733575b-548d-411c-ff30-f435103b72de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.37.4-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting botocore<1.38.0,>=1.37.4 (from boto3)\n",
            "  Downloading botocore-1.37.4-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
            "  Downloading s3transfer-0.11.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.4->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.4->boto3) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.4->boto3) (1.17.0)\n",
            "Downloading boto3-1.37.4-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.37.4-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.37.4 botocore-1.37.4 jmespath-1.0.1 s3transfer-0.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
        "\n",
        "\n",
        "# Hardcoded AWS credentials (Replace with your actual credentials)\n",
        "AWS_ACCESS_KEY_ID = 'AKIA44Y6CDCHOAYRXD5E'\n",
        "AWS_SECRET_ACCESS_KEY = 'FQIkBIqQyYkuRU6pSO8AlrwwmzNLqk0DUcdJJBaa'\n",
        "BUCKET_NAME = 'ram0604'"
      ],
      "metadata": {
        "id": "8G2XKRCG4Dev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SYl5QMP74D3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the S3 client\n",
        "s3 = boto3.client(\n",
        "       's3',\n",
        "       aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "       aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
        ")\n",
        "\n",
        "# Define the file to upload, bucket name, and S3 object name\n",
        "file_path = '/content/output/transformed_data.csv'  # Path to the file on your local machine\n",
        "bucket_name = 'ram0604'  # Name of your S3 bucket\n",
        "s3_object_name = 'result/transformed.csv'  # Name of the file in S3 (can include folders)\n",
        "\n",
        "# Upload the file\n",
        "try:\n",
        "    s3.upload_file(file_path, bucket_name, s3_object_name)\n",
        "    print(f\"File '{file_path}' uploaded to '{bucket_name}/{s3_object_name}' successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error uploading file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oThtmztF4NF9",
        "outputId": "fdf006a5-d59b-4b6b-e6db-5e5c3357e8c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File '/content/output/transformed_data.csv' uploaded to 'ram0604/result/transformed.csv' successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "\n",
        "def log(message, log_file):\n",
        "    \"\"\"Log messages to a log file with a timestamp.\"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    with open(log_file, 'a') as f:\n",
        "        f.write(f\"[{timestamp}] {message}\\n\")\n",
        "\n",
        "def extract_csv(file_path):\n",
        "    \"\"\"Extract data from a CSV file.\"\"\"\n",
        "    try:\n",
        "        return pd.read_csv(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file {file_path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def extract_json(file_path):\n",
        "    \"\"\"Extract data from a JSON file.\"\"\"\n",
        "    try:\n",
        "        return pd.read_json(file_path, lines=True)  # Use lines=True for JSON lines format\n",
        "    except ValueError as e:\n",
        "        print(f\"Error reading JSON file {file_path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def extract_xml(file_path):\n",
        "    \"\"\"Extract data from an XML file.\"\"\"\n",
        "    try:\n",
        "        tree = ET.parse(file_path)\n",
        "        root = tree.getroot()\n",
        "        all_data = []\n",
        "        for child in root:\n",
        "            data = {element.tag: element.text for element in child}\n",
        "            all_data.append(data)\n",
        "        return pd.DataFrame(all_data)\n",
        "    except ET.ParseError as e:\n",
        "        print(f\"Error reading XML file {file_path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def extract_data(data_folder):\n",
        "    \"\"\"Extract data from multiple file formats.\"\"\"\n",
        "    extracted_data = pd.DataFrame()\n",
        "    for file in glob.glob(f\"{data_folder}/*\"):\n",
        "        print(f\"Processing file: {file}\")  # Print each file being processed\n",
        "        try:\n",
        "            if file.endswith('.csv'):\n",
        "                data = extract_csv(file)\n",
        "            elif file.endswith('.json'):\n",
        "                data = extract_json(file)\n",
        "            elif file.endswith('.xml'):\n",
        "                data = extract_xml(file)\n",
        "            else:\n",
        "                continue\n",
        "            extracted_data = pd.concat([extracted_data, data], ignore_index=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file}: {e}\")\n",
        "\n",
        "    # Normalize column names to lowercase for consistency\n",
        "    extracted_data.columns = [col.lower() for col in extracted_data.columns]\n",
        "\n",
        "        # Remove duplicates\n",
        "    extracted_data = extracted_data.drop_duplicates()\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "def transform_data(data):\n",
        "    \"\"\"Transform the data (convert heights and weights).\"\"\"\n",
        "    try:\n",
        "        data['height'] = data['height'].astype(float) * 0.0254  # inches to meters\n",
        "        data['weight'] = data['weight'].astype(float) * 0.453592  # pounds to kilograms\n",
        "    except KeyError as e:\n",
        "        print(f\"Missing column during transformation: {e}\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Data conversion error during transformation: {e}\")\n",
        "    return data\n",
        "\n",
        "def load_data(data, output_file):\n",
        "    \"\"\"Save the transformed data to a CSV file.\"\"\"\n",
        "    try:\n",
        "        data.to_csv(output_file, index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving data to {output_file}: {e}\")\n",
        "\n",
        "def main():\n",
        "    # Paths\n",
        "    data_folder = r'/content/source'\n",
        "    log_file = r'/content/output/log.txt'\n",
        "    output_file = r'/content/output/transformed_data.csv'\n",
        "\n",
        "    # Clear log file\n",
        "    if os.path.exists(log_file):\n",
        "        os.remove(log_file)\n",
        "\n",
        "\n",
        "    log(\"ETL process started.\", log_file)\n",
        "\n",
        "    try:\n",
        "        # Extraction\n",
        "        log(\"Starting data extraction.\", log_file)\n",
        "        extracted_data = extract_data(data_folder)\n",
        "        log(\"Data extraction completed.\", log_file)\n",
        "\n",
        "        # Print extracted data for debugging\n",
        "        print(\"Extracted Data:\")\n",
        "        print(extracted_data.head())\n",
        "\n",
        "        # Transformation\n",
        "        log(\"Starting data transformation.\", log_file)\n",
        "        transformed_data = transform_data(extracted_data)\n",
        "        log(\"Data transformation completed.\", log_file)\n",
        "\n",
        "        # Print transformed data for debugging\n",
        "        print(\"Transformed Data:\")\n",
        "        print(transformed_data.head())\n",
        "\n",
        "        # Loading\n",
        "        log(\"Starting data loading.\", log_file)\n",
        "        load_data(transformed_data, output_file)\n",
        "        log(\"Data loading completed.\", log_file)\n",
        "\n",
        "        log(\"ETL process completed successfully.\", log_file)\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f\"ETL process failed: {e}\", log_file)\n",
        "        print(f\"ETL process failed: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdYxzrSIkcXu",
        "outputId": "893adfa7-1f91-4c46-c590-296465aea04f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: /content/source/source3.json\n",
            "Processing file: /content/source/source2.json\n",
            "Processing file: /content/source/source2.xml\n",
            "Processing file: /content/source/source1.json\n",
            "Processing file: /content/source/source3.csv\n",
            "Processing file: /content/source/source2.csv\n",
            "Processing file: /content/source/source1.csv\n",
            "Processing file: /content/source/source1.xml\n",
            "Processing file: /content/source/source3.xml\n",
            "Extracted Data:\n",
            "    name height  weight\n",
            "0   jack   68.7   123.3\n",
            "1    tom   69.8  141.49\n",
            "2  tracy  70.01  136.46\n",
            "3   john   67.9  112.37\n",
            "8  simon  67.90  112.37\n",
            "Transformed Data:\n",
            "    name    height     weight\n",
            "0   jack  1.744980  55.927894\n",
            "1    tom  1.772920  64.178732\n",
            "2  tracy  1.778254  61.897164\n",
            "3   john  1.724660  50.970133\n",
            "8  simon  1.724660  50.970133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mysql-connector-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktEO-GFvkb-L",
        "outputId": "6ea457f5-6c86-4368-b47d-875702ed2129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mysql-connector-python in /usr/local/lib/python3.11/dist-packages (9.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/output/transformed_data.csv')\n",
        "result_data = pd.DataFrame(data)\n",
        "display(result_data)"
      ],
      "metadata": {
        "id": "5onZWdfSJcI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mysqlclient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kBKUlL5MuQ4",
        "outputId": "d52ac8d6-5140-482d-9c52-bac59b47e628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mysqlclient in /usr/local/lib/python3.11/dist-packages (2.2.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymysql"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py07pZu0S5DN",
        "outputId": "afdbdd03-c900-4d21-f167-b31c24a3c1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymysql\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymysql\n",
            "Successfully installed pymysql-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Convert to DataFrame\n",
        "data = pd.read_csv('/content/output/transformed_data.csv')\n",
        "result_data = pd.DataFrame(data)\n",
        "\n",
        "# MySQL Connection Details\n",
        "DB_USER = \"admin\"\n",
        "DB_PASSWORD = \"19632000\"\n",
        "DB_HOST = \"resultdb.cpcoqoe0w4ox.eu-north-1.rds.amazonaws.com\"  # RDS host\n",
        "DB_NAME = \"resultdb1\"\n",
        "\n",
        "# Create a SQLAlchemy engine for MySQL without SSL\n",
        "engine = create_engine(\n",
        "    f\"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\"\n",
        ")\n",
        "\n",
        "# Export DataFrame to MySQL\n",
        "table_name = \"storage\"  # Replace with your desired table name\n",
        "try:\n",
        "    result_data.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
        "    print(f\"Data successfully exported to MySQL table '{table_name}'!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1GCpsqEId8N",
        "outputId": "590054f7-ef7f-4eb0-f496-551ca8277342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully exported to MySQL table 'storage'!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mysql.connector\n",
        "\n",
        "connection = mysql.connector.connect(\n",
        "  host = \"resultdb.cpcoqoe0w4ox.eu-north-1.rds.amazonaws.com\",\n",
        "  port = 3306,\n",
        "  user = \"admin\",\n",
        "  password = \"19632000\",\n",
        "  database = \"resultdb1\",\n",
        ")\n",
        "mycursor = connection.cursor()"
      ],
      "metadata": {
        "id": "Y5Rak6-kTZ7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mycursor.execute(\"SELECT * FROM storage\")\n",
        "result = mycursor.fetchall()\n",
        "# print(result)\n",
        "db_result = pd.DataFrame(result)\n",
        "display(db_result)"
      ],
      "metadata": {
        "id": "C9X8q7WaTdvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_FAg4b-IaIux"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}